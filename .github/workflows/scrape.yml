name: scrape
on:
  workflow_dispatch:

  push: 
    branches:
      - 'main'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out this repo
        uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
      - name: Installed package list
        run: apt list --installed
      - name: Remove Chrome
        run: sudo apt purge google-chrome-stable chromium-browser
      - name: Install a new Chromium
        run: sudo apt install -y chromium-browser
      
      - name: Check chromium versions
        run: |-
          which chromium-browser
          chromium-browser --version
          which chromium
          chromium --version
          which chrome
          chrome --version
          ls -al /usr/bin/chrom*
          sudo rm -rf /usr/bin/chromium && sudo ln -s /snap/bn/chromium /usr/bin/chromium

      - name: Check driver version
        run: |-
          pwd
          ls -al
          cd chromedriver
          ls -al
          chmod +x chromedriver
          bash ./update.sh
          chmod +x chromedriver && sudo cp chromedriver /usr/bin/
          chromedriver --version


      - name: Install all necessary packages
        run: pip install requests webdriver-manager selenium # beautifulsoup4 pandas

      
      - name: Run the scraping script
        run: python scraper.py

      - name: Commit and push if content changed
        run: |-
          git config user.name "Automated"
          git config user.email "actions@users.noreply.github.com"
          git add -A
          timestamp=$(date -u)
          git commit -m "Latest data: ${timestamp}" || exit 0
          git push